{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pre-training\n",
    "!python3 ../code/pretraining.py --config=\"../configs/pretraining/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run experiment\n",
    "# !python3 ../code/segmentation.py --config=\"../configs/segmentation/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run diffusion model\n",
    "# !python3 ../code/diffusion.py --config=\"../configs/diffusion/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate experiment\n",
    "# !python3 ../code/evaluate.py --config=\"../configs/segmentation/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Folds\n",
    "# 0.6587 -- original (efficientnetv2_m)\n",
    "# 0.6563 -- original (resnest101e)\n",
    "# 0.7173 -- pretraining (efficientnetv2_m)\n",
    "# 0.6870 -- pretraining (resnest101e)\n",
    "# 0.7216 -- finetuning (efficientnetv2_m)\n",
    "# 0.6922 -- finetuning (resnest101e)\n",
    "\n",
    "# # Validation folder\n",
    "# 0.6210 -- original (efficientnetv2_m)\n",
    "# 0.6310 -- original (resnest101e)\n",
    "# 0.6412 -- pretraining (efficientnetv2_m)\n",
    "# 0.6450 -- pretraining (resnest101e)\n",
    "# 0.6487 -- finetuning (efficientnetv2_m)\n",
    "# 0.6521 -- finetuning (resnest101e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create and save synthetic contrail images\n",
    "# !python3 ../code/synthesize.py --config=\"../configs/diffusion/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset import ContrailsPretrainingDataset\n",
    "from utils import data_split, normalize_min_max, load_synthetic_metadata\n",
    "\n",
    "df = data_split(\"../data/data_split.csv\")\n",
    "df_train = df[df.fold != 0]\n",
    "df_valid = df[df.fold == 0]\n",
    "\n",
    "df_synthetic_train = load_synthetic_metadata()\n",
    "df_synthetic_train = df_synthetic_train[df_synthetic_train.fold == 0]\n",
    "\n",
    "# train_dataset = ContrailsPretrainingDataset(df_train, \"../data/pseudo-labels/predictions/\", 8, 384, split=\"train\")\n",
    "valid_dataset = ContrailsPretrainingDataset(df_valid, df_synthetic_train, \"../data/pseudo-labels/predictions/\", 8, 384, split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"mask\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "axs[0].imshow(normalize_min_max(rearrange(sample[\"image\"], \"c h w -> h w c\")))\n",
    "axs[1].imshow(sample[\"mask\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "meta = pd.read_csv(\"../data/synthetic/fold-0/metadata.csv\")\n",
    "\n",
    "i = np.random.choice(meta.shape[0])\n",
    "image = cv2.imread(meta.iloc[i].image_path)\n",
    "condition = cv2.imread(meta.iloc[i].condition_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axs[0].imshow(condition)\n",
    "axs[1].imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../mids-2023/latent-diffusion\")\n",
    "sys.path.append(\"../../../mids-2023/taming-transformers\")\n",
    "sys.path.append(\"../code\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from dataset import SemanticSynthesisDataset\n",
    "from diffusion import DiffusionModule\n",
    "from einops import rearrange\n",
    "from functools import reduce\n",
    "from ldm.util import instantiate_from_config\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(img, permute=True):\n",
    "    \"\"\"Reverts a pre-processed image back to [0, 255].\"\"\"\n",
    "    x = torch.clamp((img + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "    if permute:\n",
    "        x = 255. * rearrange(x.cpu().numpy(), \"c h w -> h w c\")\n",
    "    else:\n",
    "        x = 255. * x.cpu().numpy()\n",
    "    return x.astype(np.uint8)\n",
    "\n",
    "\n",
    "def inference(model, batch, inference_steps, eta, guidance_scale):\n",
    "    seg = batch[\"segmentation\"].float()\n",
    "    bsz = seg.size(0)\n",
    "    with torch.no_grad():\n",
    "        seg = rearrange(seg, \"b h w c -> b c h w\")\n",
    "        cond = model.get_learned_conditioning(seg)\n",
    "        uncond = model.get_learned_conditioning(torch.zeros(*seg.shape).to(model.device))\n",
    "        samples, _ = model.sample_log(\n",
    "            cond=cond, \n",
    "            batch_size=bsz, \n",
    "            ddim=True,\n",
    "            ddim_steps=inference_steps, \n",
    "            eta=eta,\n",
    "            unconditional_guidance_scale=guidance_scale,\n",
    "            unconditional_conditioning=uncond\n",
    "        )\n",
    "        samples = model.decode_first_stage(samples)\n",
    "    return seg, samples\n",
    "\n",
    "\n",
    "def inference_batch(model, loader, inference_steps, eta, guidance_scale):\n",
    "    conditions = []\n",
    "    outputs = []\n",
    "    for batch in loader:\n",
    "        batch = {\"segmentation\": batch[\"segmentation\"].to(model.device)}\n",
    "        _, samples = inference(\n",
    "            model, \n",
    "            batch, \n",
    "            inference_steps=inference_steps, \n",
    "            eta=eta,\n",
    "            guidance_scale=guidance_scale\n",
    "        )\n",
    "        for i, s in enumerate(samples):\n",
    "            outputs.append(normalize_image(s))\n",
    "            conditions.append(batch[\"segmentation\"][i].cpu().numpy())\n",
    "    outputs = np.stack(outputs, axis=0)\n",
    "    conditions = np.stack(conditions, axis=0)\n",
    "    return outputs, conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "inference_steps = 500\n",
    "guidance_scale = 1.2\n",
    "eta = 1.0\n",
    "n = 16\n",
    "folds = [0]\n",
    "\n",
    "with open(\"../configs/diffusion/config.yaml\", \"rb\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "for fold in folds:\n",
    "    checkpoint_path = f\"../models/checkpoints/semantic_synthesis__fold_{fold}.ckpt\"\n",
    "\n",
    "    model = DiffusionModule(config[\"model\"])\n",
    "    model.load_state_dict(torch.load(checkpoint_path)[\"state_dict\"])\n",
    "    m = model.model.to(device)\n",
    "    m.eval()\n",
    "\n",
    "    df = pd.read_csv(\"../data/data_split.csv\")\n",
    "    df_train = df[(df.fold != fold) & (df.split != \"validation\")].iloc[:16]\n",
    "    dataset = SemanticSynthesisDataset(df_train, cond_drop_rate=0., split=\"train\")\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    inference_outputs = inference_batch(\n",
    "        m,\n",
    "        dataloader,\n",
    "        inference_steps=inference_steps,\n",
    "        eta=eta,\n",
    "        guidance_scale=guidance_scale\n",
    "    )\n",
    "\n",
    "    # output_dir = f\"/home/romainlhardy/mids-2023/data/steatosis/synthetic-outputs/{diffusion_family}/fold-{fold}/checkpoint-{checkpoint}\"\n",
    "    # os.makedirs(output_dir, exist_ok=True)\n",
    "    # os.makedirs(f\"{output_dir}/images\", exist_ok=True)\n",
    "\n",
    "    # save_outputs(inference_outputs, output_dir, diffusion_family)\n",
    "\n",
    "    # del model, dataset, dataloader, inference_outputs\n",
    "    # torch.cuda.empty_cache()\n",
    "    # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, conditions = inference_outputs\n",
    "fig, axs = plt.subplots(16, 2, figsize=(8, 16 * 4))\n",
    "for i in range(16):\n",
    "    axs[i, 0].imshow((conditions[i] * np.array([0, 60, 120, 180, 240])[None, None, :]).sum(axis=-1))\n",
    "    axs[i, 1].imshow(outputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from evaluate import load_model\n",
    "\n",
    "\n",
    "with open(\"../configs/segmentation/config.yaml\", \"rb\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "checkpoint_path = os.path.join(config[\"output_dir\"], f\"backbone_{config['model']['encoder']}__fold_0-v4.ckpt\")\n",
    "model = load_model(config[\"model\"], checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dataset import SemanticSynthesisDataset\n",
    "\n",
    "# model = load_base_model(\"../configs/diffusion/model.yaml\")\n",
    "df = pd.read_csv(\"../data/data_split.csv\")\n",
    "dataset = SemanticSynthesisDataset(df, n_labels=5, cond_drop_rate=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"image\"].min(), sample[\"image\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(24, 4))\n",
    "\n",
    "for i in range(5):\n",
    "    axs[i].imshow(sample[\"segmentation\"][..., i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample[\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from nextvit import nextvit_base\n",
    "\n",
    "m = timm.create_model(\"timm/convnextv2_base.fcmae_ft_in22k_in1k_384\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.pretrained_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "stages = [\n",
    "    nn.Identity(),\n",
    "    m.stem,\n",
    "    m.stages[0],\n",
    "    m.stages[1],\n",
    "    m.stages[2],\n",
    "    m.stages[3]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(m.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 256, 256)\n",
    "\n",
    "for block in stages:\n",
    "    x = block(x)\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from encoder import ConvNeXtEncoder, NextViTEncoder, EfficientNetEncoder, ResNestEncoder\n",
    "from model import Unet\n",
    "\n",
    "# m = NextViTEncoder(\"nextvit_base\")\n",
    "# m = timm.create_model(\"convnextv2_tiny.fcmae_ft_in22k_in1k\", pretrained=True)\n",
    "# m = ConvNeXtEncoder(\"convnextv2_base.fcmae_ft_in22k_in1k_384\", timesteps=1)\n",
    "# m = EfficientNetEncoder(\"tf_efficientnetv2_m.in21k_ft_in1k\", stage_idxs=[2, 3, 5])\n",
    "# m = EfficientNetEncoder(\"tf_efficientnetv2_s.in21k_ft_in1k\")\n",
    "# m = ResNestEncoder(\"resnest101e.in1k\", timesteps=1)\n",
    "m = Unet(\n",
    "    encoder_name=\"convnextv2_base.fcmae_ft_in22k_in1k_384\",\n",
    "    decoder_use_batchnorm=True,\n",
    "    decoder_channels=[512, 256, 128, 64, 32],\n",
    "    decoder_attention_type=\"scse\",\n",
    "    classes=1,\n",
    "    activation=None,\n",
    "    aux_params=None,\n",
    "    timesteps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "m(torch.randn(2, 1, 3, 384, 384)).shape\n",
    "# for x in m(torch.randn(2, 1, 3, 384, 384)):\n",
    "#     print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.attention1.in_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCSEModule(nn.Module):\n",
    "    def __init__(self, in_channels, timesteps=8, reduction=16):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.timesteps = timesteps\n",
    "        self.cSE = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n",
    "        self.tSE = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            nn.Conv3d(timesteps, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(1, timesteps, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.size()[-2:]\n",
    "        x = x * self.cSE(x) + x * self.sSE(x)\n",
    "        x = x.view(-1, self.timesteps, self.in_channels, h, w)\n",
    "        x = torch.sum(x * self.tSE(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "scse = SCSEModule(24)\n",
    "\n",
    "scse(torch.randn(32, 24, 192, 192)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.AdaptiveAvgPool2d(1)(torch.randn(1, 24, 192, 192)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d(24, 1, 1)(torch.randn(1, 24, 192, 192)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "x = m.stem(x)\n",
    "print(x.shape)\n",
    "for stage in m.stages:\n",
    "    x = stage(x)\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(m.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"../models/sam-encoders/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.modeling import image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "\n",
    "from encoder import SAMEncoder\n",
    "from model import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "m(torch.randn(1, 3, 384, 384)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f.shape for f in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"../models/sam-encoders/sam_vit_b_01ec64.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = image_encoder.ImageEncoderViT(\n",
    "    img_size=384,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    global_attn_indexes=[2, 5, 8, 11],\n",
    "    patch_size=16,\n",
    "    use_abs_pos=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {\n",
    "    k.replace(\"image_encoder.\", \"\"): v\n",
    "    for k, v in state_dict.items()\n",
    "    if not k.startswith(\"mask_decoder\") and not k.startswith(\"prompt_encoder\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.patch_embed(torch.randn(1, 3, 384, 384)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "int(math.log(16, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    m(torch.randn(1, 3, 384, 384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "\n",
    "import ast\n",
    "import argparse\n",
    "import cv2\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from dataset import ContrailsDataset\n",
    "from segmentation import SegmentationModule\n",
    "from torchmetrics import Dice\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import data_split, load_record, FOLDS\n",
    "\n",
    "df = data_split(\"../data/data_split.csv\")\n",
    "# df_analysis = pd.read_csv(\"../evaluation/analysis_metadata.csv\")\n",
    "\n",
    "# len(df_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_split(\"../data/data_split.csv\")\n",
    "df_train = df[df.fold != 0]\n",
    "df_valid = df[df.fold == 0]\n",
    "train_dataset = ContrailsDataset(df_train, 5, 384, split=\"train\")\n",
    "valid_dataset = ContrailsDataset(df_valid, 5, 384, split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset[0][\"image\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "row = df_analysis.iloc[i]\n",
    "prob = np.load(f\"../evaluation/predictions/{row.record_id}.npy\")\n",
    "x, y = load_record(df[df.record_id == row.record_id].iloc[0].record_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axs[0].imshow(x)\n",
    "axs[1].imshow(y)\n",
    "axs[2].imshow(prob[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_record\n",
    "\n",
    "x, y = load_record(df.iloc[0].record_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ContrailsDataset(df)\n",
    "\n",
    "sample = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"image\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "\n",
    "from encoder import Encoder \n",
    "from model import Unet\n",
    "\n",
    "# m = Encoder(\"tf_efficientnetv2_l_in21ft1k\", stage_idxs=(2, 3, 5))\n",
    "m = Unet(\n",
    "    encoder_name=\"tf_efficientnetv2_l_in21ft1k\",\n",
    "    decoder_use_batchnorm=True,\n",
    "    decoder_channels=(256, 128, 64, 32, 16),\n",
    "    decoder_attention_type=None,\n",
    "    classes=1,\n",
    "    activation=None,\n",
    "    aux_params=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# for x in m(torch.randn(1, 3, 384, 384)):\n",
    "#     print(x.shape)\n",
    "\n",
    "m(torch.randn(1, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = m.encoder.forward(torch.randn(1, 3, 256, 256))\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f[1:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.decoder.blocks[0](f[0], f[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(m.decoder.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(features):\n",
    "    features = features[1:]  # remove first skip with same spatial resolution\n",
    "    features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "    head = features[0]\n",
    "    skips = features[1:]\n",
    "\n",
    "    x = m.decoder.center(head)\n",
    "    for i, decoder_block in enumerate(m.decoder.blocks):\n",
    "        skip = skips[i] if i < len(skips) else None\n",
    "        print(x.shape, skip.shape)\n",
    "        x = decoder_block(x, skip)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model.stem(torch.randn(1, 3, 384, 384)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
