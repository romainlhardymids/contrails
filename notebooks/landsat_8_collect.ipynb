{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.misc\n",
    "import skimage.exposure\n",
    "\n",
    "from osgeo import gdal, gdalconst\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from scipy.ndimage import measurements\n",
    "from sklearn import linear_model\n",
    "from typing import Any, Dict, Iterable, List, Mapping, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BandConstants = collections.namedtuple(\"BandConstants\", [\"mult\", \"add\", \"k1\", \"k2\"])\n",
    "RADIANCE_ADD_KEY = \"RADIANCE_ADD_BAND_\"\n",
    "RADIANCE_MULT_KEY = \"RADIANCE_MULT_BAND_\"\n",
    "REFLECTANCE_ADD_KEY = \"REFLECTANCE_ADD_BAND_\"\n",
    "REFLECTANCE_MULT_KEY = \"REFLECTANCE_MULT_BAND_\"\n",
    "K1_KEY = \"K1_CONSTANT_BAND_\"\n",
    "K2_KEY = \"K2_CONSTANT_BAND_\"\n",
    "\n",
    "\n",
    "def path_from_filename(landsat_filename):\n",
    "    split_name = landsat_filename.split(\"_\")\n",
    "    path, row = split_name[2][:3], split_name[2][3:]\n",
    "    bands = split_name[0]\n",
    "    parent_dir = \"_\".join(split_name[:-1])\n",
    "    return os.path.join(f\"gs://gcp-public-data-landsat/{bands}/01/\", path, row, parent_dir, landsat_filename)\n",
    "\n",
    "\n",
    "def raster_image(tif, downsample_factor=10):\n",
    "    data = tif.ReadAsArray()\n",
    "    data = tif.ReadAsArray(\n",
    "        buf_xsize=int(data.shape[1] / downsample_factor),\n",
    "        buf_ysize=int(data.shape[0] / downsample_factor),\n",
    "        resample_alg=gdalconst.GRIORA_Average\n",
    "    )\n",
    "    data = np.where(data == 0, np.nan, data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_metadata(path_mtl):\n",
    "    if not os.path.exists(path_mtl):\n",
    "        raise OSError(\"Missing file: \" + path_mtl)\n",
    "    metadata = {}\n",
    "    with open(path_mtl) as f:\n",
    "        for line in f:\n",
    "            if \" = \" in line:\n",
    "                split_line = line.split(\" = \")\n",
    "                try:\n",
    "                    metadata[split_line[0].strip()] = float(split_line[-1].strip())\n",
    "                except ValueError:\n",
    "                    metadata[split_line[0].strip()] = split_line[-1].strip()\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def read_band_constants(metadata, band, reflectance=True):\n",
    "    if reflectance:\n",
    "        return BandConstants(\n",
    "            metadata[REFLECTANCE_MULT_KEY + band],\n",
    "            metadata[REFLECTANCE_ADD_KEY + band],\n",
    "            None, \n",
    "            None\n",
    "        )\n",
    "    else:\n",
    "        return BandConstants(\n",
    "            metadata[RADIANCE_MULT_KEY + band],\n",
    "            metadata[RADIANCE_ADD_KEY + band],\n",
    "            metadata[K1_KEY + band],\n",
    "            metadata[K2_KEY + band]\n",
    "        )\n",
    "\n",
    "\n",
    "def top_of_atmosphere_temperature(image, band_constants):\n",
    "    radiance = image * band_constants.mult\n",
    "    radiance += band_constants.add\n",
    "    denom = np.log((band_constants.k1 / radiance) + 1.)\n",
    "    return band_constants.k2 / denom\n",
    "\n",
    "\n",
    "def reflectance(image, band_constants):\n",
    "    return image * band_constants.mult + band_constants.add    \n",
    "\n",
    "\n",
    "def normalize_for_rgb(signal, bounds, adapt=True):\n",
    "    start, end = bounds\n",
    "    out = ((signal - start) / (end - start)).clip(0, 1)\n",
    "    if (np.all(np.isnan(out) | np.isclose(out, 0, atol=1e-3)) or np.all(np.isnan(out) | np.isclose(out, 1, atol=1e-3))):\n",
    "        adapt = False\n",
    "    if adapt:\n",
    "        out = skimage.exposure.equalize_adapthist(out, clip_limit=0.03)\n",
    "    return out\n",
    "\n",
    "\n",
    "def truecolor_rgb(path_r, path_g, path_b):\n",
    "    tif_r = gdal.Open(path_r)\n",
    "    tif_g = gdal.Open(path_g)\n",
    "    tif_b = gdal.Open(path_b)\n",
    "    radiance_r = raster_image(tif_r)\n",
    "    radiance_g = raster_image(tif_g)\n",
    "    radiance_b = raster_image(tif_b)\n",
    "\n",
    "    del tif_r\n",
    "    del tif_g\n",
    "    del tif_b\n",
    "    \n",
    "    rgb = np.stack([radiance_r, radiance_g, radiance_b], axis=-1)\n",
    "    rgb = rgb / (2 ** 16 - 1)\n",
    "    for i in range(3):\n",
    "        rgb[..., i] = skimage.exposure.equalize_adapthist(rgb[..., i], clip_limit=0.1)\n",
    "    return rgb\n",
    "\n",
    "\n",
    "def plot(image, ax):\n",
    "    ax.imshow(image)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "\n",
    "def get_false_color_image(path_11um, path_12um, path_1370nm, path_mtl, night=False):\n",
    "    tif_11um = gdal.Open(path_11um)\n",
    "    tif_12um = gdal.Open(path_12um)\n",
    "    tif_1370nm = gdal.Open(path_1370nm)\n",
    "\n",
    "    radiance_11um = raster_image(tif_11um)\n",
    "    radiance_12um = raster_image(tif_12um)\n",
    "    radiance_1370nm = raster_image(tif_1370nm)\n",
    "\n",
    "    del tif_11um\n",
    "    del tif_12um\n",
    "    del tif_1370nm\n",
    "\n",
    "    metadata = parse_metadata(path_mtl)\n",
    "    constants_11um = read_band_constants(metadata, \"10\", reflectance=False)\n",
    "    constants_12um = read_band_constants(metadata, \"11\", reflectance=False)\n",
    "    constants_1370nm = read_band_constants(metadata, \"9\", reflectance=True)  \n",
    "\n",
    "    temperature_11um = top_of_atmosphere_temperature(radiance_11um, constants_11um)\n",
    "    temperature_12um = top_of_atmosphere_temperature(radiance_12um, constants_12um)\n",
    "    reflectance_1370nm = reflectance(radiance_1370nm, constants_1370nm)\n",
    "\n",
    "    tdiff = temperature_11um - temperature_12um\n",
    "\n",
    "    tdiff_clip = (-1, 5.5)  \n",
    "    tdiff_bounds = (-tdiff_clip[1], -tdiff_clip[0])\n",
    "    ir_r = normalize_for_rgb(-1 * tdiff, tdiff_bounds, adapt=False)\n",
    "\n",
    "    if night:\n",
    "        ir_g = np.zeros_like(reflectance_1370nm)\n",
    "        t12_bounds = (243, 303)  # From a sample of nighttime landsat scenes.\n",
    "    else:\n",
    "        ir_g = normalize_for_rgb(1 - reflectance_1370nm, bounds=(0.8, 1))\n",
    "        t12_bounds = (283, 303)  # From a sample of daytime landsat scenes.\n",
    "\n",
    "    ir_b = normalize_for_rgb(temperature_12um, t12_bounds)\n",
    "    \n",
    "    return np.stack([ir_r, ir_g, ir_b], axis=-1), temperature_11um, temperature_12um, reflectance_1370nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWELVE_MICRONS = \"temperature_12um\"\n",
    "TDIFF = \"brightness_temperature_difference\"\n",
    "\n",
    "def lowpass(image, config):\n",
    "    kernel_size_pixels = config[\"lowpass_kernel_size_pixels\"]\n",
    "    pixels_per_sigma = config[\"lowpass_kernel_pixels_per_sigma\"]\n",
    "    units_per_sigma = pixels_per_sigma / kernel_size_pixels\n",
    "    n1, n2 = np.meshgrid(\n",
    "        np.linspace(-1, 1, kernel_size_pixels),\n",
    "        np.linspace(-1, 1, kernel_size_pixels)\n",
    "    )\n",
    "    distance_from_center = np.sqrt(np.square(n1) + np.square(n2))\n",
    "    gaussian = (1 / (units_per_sigma * np.sqrt(2 * np.pi)) * (np.exp(-np.square(distance_from_center / units_per_sigma) / 2)))\n",
    "    gaussian_kernel = gaussian / gaussian.sum()\n",
    "    return signal.convolve2d(image, gaussian_kernel, mode=\"same\")\n",
    "\n",
    "\n",
    "def synthesize_line_kernel(degrees, config):\n",
    "    size = config[\"line_kernel_size_pixels\"]\n",
    "    radians = degrees * (np.pi / 180.)\n",
    "    n1, n2 = np.meshgrid(np.linspace(-1, 1, size), np.linspace(-1, 1, size))\n",
    "    d = (n1 * np.cos(radians)) - (n2 * np.sin(radians))\n",
    "    pixels_per_sigma1 = config[\"line_kernel_pixels_per_sigma1\"]\n",
    "    pixels_per_sigma2 = (pixels_per_sigma1 + config[\"line_kernel_pixels_per_sigma_delta\"])\n",
    "    units_per_sigma1 = (pixels_per_sigma1 / config[\"line_kernel_size_pixels\"])\n",
    "    gaussian1 = ((1 / (units_per_sigma1 * np.sqrt(2 * np.pi))) * (np.exp(-np.square(d / units_per_sigma1) / 2)))\n",
    "    units_per_sigma2 = (pixels_per_sigma2 / config[\"line_kernel_size_pixels\"])\n",
    "    gaussian2 = ((1 / (units_per_sigma2 * np.sqrt(2 * np.pi))) * (np.exp(-np.square(d / units_per_sigma2) / 2)))\n",
    "    difference_of_gaussians = gaussian1 - gaussian2\n",
    "    return zero_sum_normalize(difference_of_gaussians)\n",
    "\n",
    "\n",
    "def zero_sum_normalize(kernel: np.ndarray) -> np.ndarray:\n",
    "    pos_sum = np.sum(np.where(kernel > 0, kernel, 0))\n",
    "    neg_sum = -np.sum(np.where(kernel <= 0, kernel, 0))\n",
    "    return np.where(kernel > 0, kernel / pos_sum, kernel / neg_sum)\n",
    "\n",
    "\n",
    "def get_regional_gradient_mask(\n",
    "    t_12um: np.ndarray,\n",
    "    t_12um_stddev: np.ndarray,\n",
    "    config: Mapping[str, Any]\n",
    "):\n",
    "    mean_kernel = np.ones(config[\"prewitt_operator_size_pixels\"], dtype=np.int32)\n",
    "    gradient_kernel = np.concatenate([\n",
    "        np.ones(config[\"prewitt_operator_smoothing_pixels\"], dtype=np.int32),\n",
    "        np.zeros(config[\"prewitt_operator_size_pixels\"] - 2 * config[\"prewitt_operator_smoothing_pixels\"], dtype=np.int32),\n",
    "        -np.ones(config[\"prewitt_operator_smoothing_pixels\"], dtype=np.int32)\n",
    "    ])\n",
    "    prewitt_col = np.outer(mean_kernel, gradient_kernel)\n",
    "    prewitt_col = zero_sum_normalize(prewitt_col)\n",
    "    prewitt_row = prewitt_col.T\n",
    "    gradient_row = signal.convolve2d(t_12um, prewitt_row, mode=\"same\")\n",
    "    gradient_col = signal.convolve2d(t_12um, prewitt_col, mode=\"same\")\n",
    "    t_12um_regional_gradient = np.sqrt(np.square(gradient_row) + np.square(gradient_col))\n",
    "    return (t_12um_regional_gradient < (config[\"regional_gradient_max_scale\"] * t_12um_stddev) + config[\"regional_gradient_max_offset\"])\n",
    "\n",
    "\n",
    "def mannstein_one_angle_mask(features, config, degrees):\n",
    "    t_12um = features[TWELVE_MICRONS]\n",
    "    difference = features[TDIFF]\n",
    "\n",
    "    t_12um_inverse = -t_12um\n",
    "\n",
    "    t_12um_inverse_smoothed = lowpass(t_12um_inverse, config)\n",
    "    difference_smoothed = lowpass(difference, config)\n",
    "\n",
    "    t_12um_inverse_signal = t_12um_inverse - t_12um_inverse_smoothed\n",
    "    difference_signal = difference - difference_smoothed\n",
    "\n",
    "    t_12um_inverse_stddev = lowpass(np.sqrt(np.square(t_12um_inverse_signal)), config)\n",
    "    difference_stddev = lowpass(np.sqrt(np.square(difference_signal)), config)\n",
    "\n",
    "    t_12um_inverse_normalized = t_12um_inverse_signal / (t_12um_inverse_stddev + config[\"stddev_epsilon\"])\n",
    "    difference_normalized = difference_signal / (difference_stddev + config[\"stddev_epsilon\"])\n",
    "\n",
    "    t_12um_inverse_clipped = np.clip(t_12um_inverse_normalized, -config[\"normalized_clip_magnitude\"], config[\"normalized_clip_magnitude\"])\n",
    "    difference_clipped = np.clip(difference_normalized, -config[\"normalized_clip_magnitude\"], config[\"normalized_clip_magnitude\"])\n",
    "\n",
    "    normalized_sum = t_12um_inverse_clipped + difference_clipped\n",
    "\n",
    "    line_kernel = synthesize_line_kernel(degrees, config)\n",
    "    detected_lines = signal.convolve2d(normalized_sum, line_kernel, mode='same')\n",
    "\n",
    "    if \"regional_gradient_max_scale\" in config:\n",
    "        regional_gradient_mask = get_regional_gradient_mask(t_12um, t_12um_inverse_stddev, config)\n",
    "    else:\n",
    "        regional_gradient_mask = np.ones_like(detected_lines, dtype=np.bool)\n",
    "\n",
    "    normalized_sum = np.where(np.isfinite(detected_lines), normalized_sum, np.nan)\n",
    "    difference = np.where(np.isfinite(detected_lines), difference, np.nan)\n",
    "    regional_gradient_mask = np.where(np.isfinite(detected_lines), regional_gradient_mask, False)\n",
    "\n",
    "    line_mask = (\n",
    "        (detected_lines > config[\"normalized_sum_threshold\"]) &\n",
    "        (normalized_sum > config[\"normalized_sum_threshold\"]) &\n",
    "        (difference > config[\"temperature_difference_threshold\"]) &\n",
    "        regional_gradient_mask\n",
    "    )\n",
    "\n",
    "    line_mask = np.where(np.isfinite(detected_lines), line_mask, np.nan)\n",
    "    return line_mask\n",
    "\n",
    "\n",
    "def get_angles(config):\n",
    "    return np.linspace(0, 180, config[\"num_line_kernels\"], endpoint=False)\n",
    "\n",
    "\n",
    "def label_blobs(line_mask, config):\n",
    "    line_mask = np.where(np.isnan(line_mask), 0, line_mask)\n",
    "    labels, _ = measurements.label(line_mask, structure=np.ones((3, 3)))\n",
    "    bincount = np.bincount(labels.flatten())\n",
    "    right_sized_labels, = (\n",
    "        (bincount > config[\"min_contrail_num_pixels\"]) &\n",
    "        (bincount < config[\"max_contrail_num_pixels\"])\n",
    "    ).nonzero()\n",
    "\n",
    "    for label in right_sized_labels:\n",
    "        yield (labels == label).nonzero()\n",
    "\n",
    "\n",
    "def linear_fit(xs, ys):\n",
    "    train_ys = ys.reshape(-1, 1)\n",
    "    train_xs = xs.reshape(-1, 1)\n",
    "    regression = linear_model.LinearRegression().fit(train_xs, train_ys)\n",
    "    return (regression.score(train_xs, train_ys), regression.coef_[0][0], regression.intercept_[0])\n",
    "\n",
    "\n",
    "def linear_and_long_enough(ys, xs, config, avg_size=3, round_to_int=True):\n",
    "    sorted_indices = np.argsort(ys)\n",
    "    y1 = np.mean(ys[sorted_indices][:avg_size])\n",
    "    y2 = np.mean(ys[sorted_indices][-avg_size:])\n",
    "    sorted_indices = np.argsort(xs)\n",
    "    x1 = np.mean(xs[sorted_indices][:avg_size])\n",
    "    x2 = np.mean(xs[sorted_indices][-avg_size:])\n",
    "\n",
    "    if abs(x1 - x2) > abs(y1 - y2):\n",
    "        score, slope, intercept = linear_fit(xs=xs, ys=ys)\n",
    "        y1 = x1 * slope + intercept\n",
    "        y2 = x2 * slope + intercept\n",
    "        pixel_length = np.sqrt(np.square(x1 - x2) + np.square(y1 - y2))\n",
    "    else:\n",
    "        score, slope, intercept = linear_fit(xs=ys, ys=xs)\n",
    "        x1 = y1 * slope + intercept\n",
    "        x2 = y2 * slope + intercept\n",
    "        pixel_length = np.sqrt(np.square(y1 - y2) + np.square(x1 - x2))\n",
    "\n",
    "    return (score > np.square(config[\"min_linear_regression_r_score\"]) and pixel_length > config[\"min_contrail_pixel_length\"])\n",
    "\n",
    "\n",
    "def find_contrail_pixels(line_mask, config):\n",
    "    contrail_pixel_coords = []\n",
    "    for rows, cols in label_blobs(line_mask, config):\n",
    "        if not linear_and_long_enough(rows, cols, config):\n",
    "            continue\n",
    "        contrail_pixel_coords.append((rows, cols))\n",
    "    return contrail_pixel_coords        \n",
    "\n",
    "\n",
    "def mannstein_contrail_mask(features, config):\n",
    "    contrail_mask = np.zeros_like(features[TDIFF], dtype=np.int32)\n",
    "    for degrees in get_angles(config):\n",
    "        line_mask = mannstein_one_angle_mask(features, config, degrees)\n",
    "        for rows, cols in find_contrail_pixels(line_mask, config):\n",
    "            contrail_mask[rows, cols] = 1\n",
    "    return contrail_mask     \n",
    "\n",
    "config = {\n",
    "  \"stddev_epsilon\": 0.1,\n",
    "  \"normalized_clip_magnitude\": 2.0,\n",
    "  \"normalized_sum_threshold\": 1,            \n",
    "  \"temperature_difference_threshold\": 1.33,\n",
    "  \"max_contrail_num_pixels\": 1000,\n",
    "  \"min_contrail_num_pixels\": 11,             \n",
    "  \"min_contrail_pixel_length\": 14,           \n",
    "  \"min_linear_regression_r_score\": 0.25,\n",
    "  \"num_line_kernels\": 16,                   \n",
    "  \"line_kernel_size_pixels\": 19,\n",
    "  \"line_kernel_pixels_per_sigma1\": 1,        \n",
    "  \"line_kernel_pixels_per_sigma_delta\": 2, \n",
    "  \"lowpass_kernel_pixels_per_sigma\": 3.88,  \n",
    "  \"lowpass_kernel_size_pixels\": 5,\n",
    "  \"prewitt_operator_size_pixels\": 15,        \n",
    "  \"prewitt_operator_smoothing_pixels\": 3,\n",
    "  \"regional_gradient_max_scale\": 2.0,\n",
    "  \"regional_gradient_max_offset\": 1.0        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/romainlhardy/kaggle/contrails/data/landsat-8/\"\n",
    "dates = [\"2021_06_11_1623455786\", \"2023_01_20_1674247800\"]\n",
    "tmp_dir = \"./tmp_data/\"\n",
    "save_dir = \"/home/romainlhardy/kaggle/contrails/data/landsat-8/processed\"\n",
    "\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "rows = []\n",
    "for date in dates:\n",
    "    for i in range(0, 100):\n",
    "        file_name = os.path.join(data_dir, date, \"data\", f\"landsat_contrails.json-000{i:02d}-of-00100\")\n",
    "        # file_name = \"/home/romainlhardy/kaggle/contrails/data/landsat-8/2021_06_11_1623455786/data/demo_shard.json\"\n",
    "        print(file_name)\n",
    "        with open(file_name, \"r\") as f:\n",
    "            for json_obj in f:\n",
    "                scene_data = json.loads(json_obj)\n",
    "                filename_11um = scene_data[\"filename\"]\n",
    "                # filename_11um = \"LC08_L1TP_036037_20180406_20180417_01_T1_B10.TIF\"\n",
    "                filename_12um = filename_11um.replace(\"B10.TIF\", \"B11.TIF\")\n",
    "                filename_1370nm = filename_11um.replace(\"B10.TIF\", \"B9.TIF\")\n",
    "                filename_red = filename_11um.replace(\"B10.TIF\", \"B4.TIF\")\n",
    "                filename_green = filename_11um.replace(\"B10.TIF\", \"B3.TIF\")\n",
    "                filename_blue = filename_11um.replace(\"B10.TIF\", \"B2.TIF\")\n",
    "                filename_mtl = filename_11um.replace(\"B10.TIF\", \"MTL.txt\")\n",
    "\n",
    "                !gsutil cp {path_from_filename(filename_11um)} {tmp_dir}\n",
    "                !gsutil cp {path_from_filename(filename_12um)} {tmp_dir}\n",
    "                !gsutil cp {path_from_filename(filename_1370nm)} {tmp_dir}\n",
    "                !gsutil cp {path_from_filename(filename_red)} {tmp_dir}\n",
    "                !gsutil cp {path_from_filename(filename_green)} {tmp_dir}\n",
    "                !gsutil cp {path_from_filename(filename_blue)} {tmp_dir}\n",
    "                !gsutil cp {path_from_filename(filename_mtl)} {tmp_dir}\n",
    "\n",
    "                path_11um = os.path.join(tmp_dir, filename_11um)\n",
    "                path_12um = os.path.join(tmp_dir, filename_12um)\n",
    "                path_1370nm = os.path.join(tmp_dir, filename_1370nm)\n",
    "                path_red = os.path.join(tmp_dir, filename_red)\n",
    "                path_green = os.path.join(tmp_dir, filename_green)\n",
    "                path_blue = os.path.join(tmp_dir, filename_blue)\n",
    "                path_mtl = os.path.join(tmp_dir, filename_mtl)\n",
    "\n",
    "                false_color_image, temperature_11um, temperature_12um, reflectance_1370nm = get_false_color_image(path_11um, path_12um, path_1370nm, path_mtl)\n",
    "                features = {\n",
    "                    TWELVE_MICRONS: temperature_12um,\n",
    "                    TDIFF: temperature_11um - temperature_12um,\n",
    "                }\n",
    "                mannstein = mannstein_contrail_mask(features, config)   \n",
    "                mask = np.zeros(false_color_image.shape[:2], dtype=np.uint8)\n",
    "                patches = []\n",
    "                for polygon in scene_data[\"polygons\"]:\n",
    "                    cv2.fillConvexPoly(mask, np.array(polygon).astype(np.int32), 1) \n",
    "\n",
    "                hash = random.getrandbits(128)\n",
    "                img_path = os.path.join(save_dir, \"images\", f\"{hash}.npy\")\n",
    "                msk_path = os.path.join(save_dir, \"masks\", f\"{hash}.npy\")\n",
    "                man_path = os.path.join(save_dir, \"mannstein\", f\"{hash}.npy\")\n",
    "\n",
    "                np.save(img_path, false_color_image)\n",
    "                np.save(msk_path, mask)\n",
    "                np.save(man_path, mannstein)\n",
    "\n",
    "                row = {\n",
    "                    \"record_id\": hash,\n",
    "                    \"index\": i,\n",
    "                    \"date\": date,\n",
    "                    \"image_path\": img_path,\n",
    "                    \"mask_path\": msk_path,\n",
    "                    \"mannstein_path\": man_path\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "                !rm {path_11um}\n",
    "                !rm {path_12um}\n",
    "                !rm {path_1370nm}\n",
    "                !rm {path_red}\n",
    "                !rm {path_green}\n",
    "                !rm {path_blue}\n",
    "                !rm {path_mtl}\n",
    "\n",
    "os.rmdir(tmp_dir)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(os.path.join(save_dir, \"metadata.csv\"), index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
